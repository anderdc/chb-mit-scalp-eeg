{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c55958d8-f184-45ef-9edc-629a2e25540b",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "Time to create an NN that will (hopefully) perform much better than a support vector machine. I intend to use a simple deep neural network.\n",
    "\n",
    "\n",
    "### Goals \n",
    "- refresh myself on creating a straightforward neural network using pytorch\n",
    "- gain insight on the performance of a basic NN\n",
    "- perform hyperparameter tuning\n",
    "\n",
    "### Results\n",
    "- First attempt involves an nn with`4` hidden layers. Even using class weighting, I can't get the model to predict a 'true' or ictal state. This was the same issue I face with the SVM, leading to a misleading high accuracy. When you uncover some other metrics (recall, precision, F-1) it's clear to see that the model just predicts 0, and there are only false negatives.\n",
    "\n",
    "\n",
    "### Notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d5de65cd-56fd-4dac-b9f3-4431679306d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import pandas as pd\n",
    "import extraction\n",
    "import importlib\n",
    "from collections import Counter\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from extraction.LT import LTPipeline\n",
    "importlib.reload(extraction.LT) # bc I keep updating LTPipeine, this ensures the ipynb cache updates\n",
    "from extraction.tools import get_all_edf_files_for_patient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc6a257-1f36-423f-a589-4bc3ee7e640a",
   "metadata": {},
   "source": [
    "### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39233c50-8e46-4e53-b1b1-2f70452c38d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37m2025-07-04 00:12:39,044 - INFO - 43 total file(s) in pipeline!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Train Test Split results ********************\n",
      "X_train shape: (116748, 345)\n",
      "X_test shape: (2879, 345)\n",
      "y_train shape: (116748,)\n",
      "y_test shape: (2879,)\n"
     ]
    }
   ],
   "source": [
    "# pipeline\n",
    "files = get_all_edf_files_for_patient('chb01')\n",
    "# files.extend(get_all_edf_files_for_patient('chb02'))\n",
    "# files.extend(get_all_edf_files_for_patient('chb03'))\n",
    "files.extend(['chb15_06.edf'])   # manually add one file that will be validation data\n",
    "\n",
    "# print(files)\n",
    "pipeline = LTPipeline(files)  \n",
    "\n",
    "# TODO: processing multiple files in parallel doesn't seem to help like I think it would... I might have to do some timing/fixing on this\n",
    "X_train, X_test, y_train, y_test = pipeline.train_test_split(validation_patient_id='chb15')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb6500c4-1268-4166-a7a0-cc78b4906d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67a5fff3-ee4d-48aa-91b4-cab8b4658936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)    # long = 64-bit int\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# wrap X, y together\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# DataLoaders for easier training\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # purposely shuffling train data set\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # test dataset is always doing 'leave one patient out'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa942ea-395e-4a09-8b84-524a51d795df",
   "metadata": {},
   "source": [
    "### NN 1\n",
    "- 4 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6688826b-9937-4523-9df7-b34eb1f219c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture\n",
    "class ChbMitNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChbMitNN, self).__init__()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(345, 450)  # 345 features\n",
    "        self.fc2 = nn.Linear(450, 600)\n",
    "        self.fc3 = nn.Linear(600, 500)\n",
    "        self.fc4 = nn.Linear(500, 350)\n",
    "        self.fc5 = nn.Linear(350, 100)\n",
    "        self.fc6 = nn.Linear(100, 2)  # binary classification\n",
    "\n",
    "        # dropout for generalization\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)     \n",
    "\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc6(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0e97bde1-9fab-4fd9-80f8-8eb8a5fa7879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChbMitNN(\n",
       "  (fc1): Linear(in_features=345, out_features=450, bias=True)\n",
       "  (fc2): Linear(in_features=450, out_features=600, bias=True)\n",
       "  (fc3): Linear(in_features=600, out_features=500, bias=True)\n",
       "  (fc4): Linear(in_features=500, out_features=350, bias=True)\n",
       "  (fc5): Linear(in_features=350, out_features=100, bias=True)\n",
       "  (fc6): Linear(in_features=100, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate model, define loss function, define optimizer, move to gpu (if available)\n",
    "model = ChbMitNN()\n",
    "\n",
    "# manually calculate inverse frequency weights for class weighting\n",
    "counts = Counter(y_train_tensor.tolist())\n",
    "total = sum(counts.values())\n",
    "weight0 = total / (2 * counts[0])\n",
    "weight1 = total / (2 * counts[1])\n",
    "\n",
    "weights = torch.tensor([weight0, weight1], dtype=torch.float32).to(device) # bc our dataset is so skewed to inter-ictal, heavily weight for ictal states\n",
    "loss_func = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "db6aa4e5-bc4c-40e2-b404-32ab5545c114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 0.773\n",
      "Epoch 1, Batch 200, Loss: 0.333\n",
      "Epoch 1, Batch 300, Loss: 0.159\n",
      "Epoch 1, Batch 400, Loss: 0.267\n",
      "Epoch 1, Batch 500, Loss: 0.390\n",
      "Epoch 1, Batch 600, Loss: 0.138\n",
      "Epoch 1, Batch 700, Loss: 0.092\n",
      "Epoch 1, Batch 800, Loss: 0.517\n",
      "Epoch 1, Batch 900, Loss: 0.501\n",
      "Epoch 1, Batch 1000, Loss: 0.226\n",
      "Epoch 1, Batch 1100, Loss: 0.132\n",
      "Epoch 1, Batch 1200, Loss: 0.335\n",
      "Epoch 1, Batch 1300, Loss: 0.333\n",
      "Epoch 1, Batch 1400, Loss: 0.468\n",
      "Epoch 1, Batch 1500, Loss: 0.208\n",
      "Epoch 1, Batch 1600, Loss: 0.176\n",
      "Epoch 1, Batch 1700, Loss: 0.092\n",
      "Epoch 1, Batch 1800, Loss: 0.068\n",
      "Epoch 2, Batch 100, Loss: 0.663\n",
      "Epoch 2, Batch 200, Loss: 0.247\n",
      "Epoch 2, Batch 300, Loss: 0.588\n",
      "Epoch 2, Batch 400, Loss: 0.094\n",
      "Epoch 2, Batch 500, Loss: 0.264\n",
      "Epoch 2, Batch 600, Loss: 0.224\n",
      "Epoch 2, Batch 700, Loss: 0.262\n",
      "Epoch 2, Batch 800, Loss: 0.314\n",
      "Epoch 2, Batch 900, Loss: 0.256\n",
      "Epoch 2, Batch 1000, Loss: 0.929\n",
      "Epoch 2, Batch 1100, Loss: 0.175\n",
      "Epoch 2, Batch 1200, Loss: 0.066\n",
      "Epoch 2, Batch 1300, Loss: 0.789\n",
      "Epoch 2, Batch 1400, Loss: 0.544\n",
      "Epoch 2, Batch 1500, Loss: 1.052\n",
      "Epoch 2, Batch 1600, Loss: 1.887\n",
      "Epoch 2, Batch 1700, Loss: 8.203\n",
      "Epoch 2, Batch 1800, Loss: 1.235\n",
      "Epoch 3, Batch 100, Loss: 3.555\n",
      "Epoch 3, Batch 200, Loss: 0.562\n",
      "Epoch 3, Batch 300, Loss: 0.340\n",
      "Epoch 3, Batch 400, Loss: 1.962\n",
      "Epoch 3, Batch 500, Loss: 0.351\n",
      "Epoch 3, Batch 600, Loss: 2.711\n",
      "Epoch 3, Batch 700, Loss: 2.885\n",
      "Epoch 3, Batch 800, Loss: 0.122\n",
      "Epoch 3, Batch 900, Loss: 1.090\n",
      "Epoch 3, Batch 1000, Loss: 0.181\n",
      "Epoch 3, Batch 1100, Loss: 0.117\n",
      "Epoch 3, Batch 1200, Loss: 0.431\n",
      "Epoch 3, Batch 1300, Loss: 0.395\n",
      "Epoch 3, Batch 1400, Loss: 0.181\n",
      "Epoch 3, Batch 1500, Loss: 0.163\n",
      "Epoch 3, Batch 1600, Loss: 0.140\n",
      "Epoch 3, Batch 1700, Loss: 0.463\n",
      "Epoch 3, Batch 1800, Loss: 0.164\n",
      "Epoch 4, Batch 100, Loss: 0.501\n",
      "Epoch 4, Batch 200, Loss: 0.167\n",
      "Epoch 4, Batch 300, Loss: 0.153\n",
      "Epoch 4, Batch 400, Loss: 0.205\n",
      "Epoch 4, Batch 500, Loss: 0.213\n",
      "Epoch 4, Batch 600, Loss: 0.190\n",
      "Epoch 4, Batch 700, Loss: 0.563\n",
      "Epoch 4, Batch 800, Loss: 0.192\n",
      "Epoch 4, Batch 900, Loss: 0.137\n",
      "Epoch 4, Batch 1000, Loss: 0.696\n",
      "Epoch 4, Batch 1100, Loss: 2.867\n",
      "Epoch 4, Batch 1200, Loss: 0.109\n",
      "Epoch 4, Batch 1300, Loss: 4.853\n",
      "Epoch 4, Batch 1400, Loss: 0.298\n",
      "Epoch 4, Batch 1500, Loss: 0.274\n",
      "Epoch 4, Batch 1600, Loss: 0.344\n",
      "Epoch 4, Batch 1700, Loss: 0.171\n",
      "Epoch 4, Batch 1800, Loss: 0.196\n",
      "Epoch 5, Batch 100, Loss: 0.170\n",
      "Epoch 5, Batch 200, Loss: 10.361\n",
      "Epoch 5, Batch 300, Loss: 0.162\n",
      "Epoch 5, Batch 400, Loss: 0.294\n",
      "Epoch 5, Batch 500, Loss: 0.166\n",
      "Epoch 5, Batch 600, Loss: 0.397\n",
      "Epoch 5, Batch 700, Loss: 0.489\n",
      "Epoch 5, Batch 800, Loss: 0.147\n",
      "Epoch 5, Batch 900, Loss: 0.425\n",
      "Epoch 5, Batch 1000, Loss: 2.600\n",
      "Epoch 5, Batch 1100, Loss: 0.377\n",
      "Epoch 5, Batch 1200, Loss: 0.114\n",
      "Epoch 5, Batch 1300, Loss: 5.345\n",
      "Epoch 5, Batch 1400, Loss: 1.409\n",
      "Epoch 5, Batch 1500, Loss: 4.939\n",
      "Epoch 5, Batch 1600, Loss: 0.470\n",
      "Epoch 5, Batch 1700, Loss: 0.177\n",
      "Epoch 5, Batch 1800, Loss: 10.350\n",
      "Epoch 6, Batch 100, Loss: 28.013\n",
      "Epoch 6, Batch 200, Loss: 0.394\n",
      "Epoch 6, Batch 300, Loss: 0.372\n",
      "Epoch 6, Batch 400, Loss: 0.228\n",
      "Epoch 6, Batch 500, Loss: 0.147\n",
      "Epoch 6, Batch 600, Loss: 0.148\n",
      "Epoch 6, Batch 700, Loss: 0.205\n",
      "Epoch 6, Batch 800, Loss: 0.154\n",
      "Epoch 6, Batch 900, Loss: 0.131\n",
      "Epoch 6, Batch 1000, Loss: 0.331\n",
      "Epoch 6, Batch 1100, Loss: 0.219\n",
      "Epoch 6, Batch 1200, Loss: 0.207\n",
      "Epoch 6, Batch 1300, Loss: 10.007\n",
      "Epoch 6, Batch 1400, Loss: 0.579\n",
      "Epoch 6, Batch 1500, Loss: 1.060\n",
      "Epoch 6, Batch 1600, Loss: 0.663\n",
      "Epoch 6, Batch 1700, Loss: 0.131\n",
      "Epoch 6, Batch 1800, Loss: 0.117\n",
      "Epoch 7, Batch 100, Loss: 0.517\n",
      "Epoch 7, Batch 200, Loss: 0.149\n",
      "Epoch 7, Batch 300, Loss: 0.130\n",
      "Epoch 7, Batch 400, Loss: 0.131\n",
      "Epoch 7, Batch 500, Loss: 0.114\n",
      "Epoch 7, Batch 600, Loss: 0.243\n",
      "Epoch 7, Batch 700, Loss: 0.257\n",
      "Epoch 7, Batch 800, Loss: 5.036\n",
      "Epoch 7, Batch 900, Loss: 0.250\n",
      "Epoch 7, Batch 1000, Loss: 15.318\n",
      "Epoch 7, Batch 1100, Loss: 0.430\n",
      "Epoch 7, Batch 1200, Loss: 0.140\n",
      "Epoch 7, Batch 1300, Loss: 0.117\n",
      "Epoch 7, Batch 1400, Loss: 0.071\n",
      "Epoch 7, Batch 1500, Loss: 1.080\n",
      "Epoch 7, Batch 1600, Loss: 0.102\n",
      "Epoch 7, Batch 1700, Loss: 0.431\n",
      "Epoch 7, Batch 1800, Loss: 0.263\n",
      "Epoch 8, Batch 100, Loss: 0.082\n",
      "Epoch 8, Batch 200, Loss: 0.084\n",
      "Epoch 8, Batch 300, Loss: 0.306\n",
      "Epoch 8, Batch 400, Loss: 0.077\n",
      "Epoch 8, Batch 500, Loss: 0.911\n",
      "Epoch 8, Batch 600, Loss: 0.101\n",
      "Epoch 8, Batch 700, Loss: 0.505\n",
      "Epoch 8, Batch 800, Loss: 0.082\n",
      "Epoch 8, Batch 900, Loss: 0.176\n",
      "Epoch 8, Batch 1000, Loss: 0.181\n",
      "Epoch 8, Batch 1100, Loss: 0.136\n",
      "Epoch 8, Batch 1200, Loss: 0.082\n",
      "Epoch 8, Batch 1300, Loss: 0.080\n",
      "Epoch 8, Batch 1400, Loss: 0.083\n",
      "Epoch 8, Batch 1500, Loss: 0.198\n",
      "Epoch 8, Batch 1600, Loss: 0.095\n",
      "Epoch 8, Batch 1700, Loss: 0.616\n",
      "Epoch 8, Batch 1800, Loss: 0.075\n",
      "Epoch 9, Batch 100, Loss: 0.081\n",
      "Epoch 9, Batch 200, Loss: 2.136\n",
      "Epoch 9, Batch 300, Loss: 0.170\n",
      "Epoch 9, Batch 400, Loss: 0.211\n",
      "Epoch 9, Batch 500, Loss: 2.930\n",
      "Epoch 9, Batch 600, Loss: 1.607\n",
      "Epoch 9, Batch 700, Loss: 14.510\n",
      "Epoch 9, Batch 800, Loss: 0.353\n",
      "Epoch 9, Batch 900, Loss: 0.079\n",
      "Epoch 9, Batch 1000, Loss: 0.272\n",
      "Epoch 9, Batch 1100, Loss: 0.499\n",
      "Epoch 9, Batch 1200, Loss: 1.045\n",
      "Epoch 9, Batch 1300, Loss: 0.222\n",
      "Epoch 9, Batch 1400, Loss: 2.854\n",
      "Epoch 9, Batch 1500, Loss: 0.124\n",
      "Epoch 9, Batch 1600, Loss: 2.952\n",
      "Epoch 9, Batch 1700, Loss: 0.089\n",
      "Epoch 9, Batch 1800, Loss: 0.743\n",
      "Epoch 10, Batch 100, Loss: 0.410\n",
      "Epoch 10, Batch 200, Loss: 0.133\n",
      "Epoch 10, Batch 300, Loss: 0.087\n",
      "Epoch 10, Batch 400, Loss: 0.076\n",
      "Epoch 10, Batch 500, Loss: 0.208\n",
      "Epoch 10, Batch 600, Loss: 0.316\n",
      "Epoch 10, Batch 700, Loss: 0.092\n",
      "Epoch 10, Batch 800, Loss: 0.844\n",
      "Epoch 10, Batch 900, Loss: 0.080\n",
      "Epoch 10, Batch 1000, Loss: 0.578\n",
      "Epoch 10, Batch 1100, Loss: 0.082\n",
      "Epoch 10, Batch 1200, Loss: 0.307\n",
      "Epoch 10, Batch 1300, Loss: 4.120\n",
      "Epoch 10, Batch 1400, Loss: 0.069\n",
      "Epoch 10, Batch 1500, Loss: 0.039\n",
      "Epoch 10, Batch 1600, Loss: 0.463\n",
      "Epoch 10, Batch 1700, Loss: 0.064\n",
      "Epoch 10, Batch 1800, Loss: 0.055\n",
      "Epoch 11, Batch 100, Loss: 0.038\n",
      "Epoch 11, Batch 200, Loss: 0.150\n",
      "Epoch 11, Batch 300, Loss: 0.093\n",
      "Epoch 11, Batch 400, Loss: 0.049\n",
      "Epoch 11, Batch 500, Loss: 5.704\n",
      "Epoch 11, Batch 600, Loss: 0.434\n",
      "Epoch 11, Batch 700, Loss: 0.370\n",
      "Epoch 11, Batch 800, Loss: 0.356\n",
      "Epoch 11, Batch 900, Loss: 5.395\n",
      "Epoch 11, Batch 1000, Loss: 0.380\n",
      "Epoch 11, Batch 1100, Loss: 0.291\n",
      "Epoch 11, Batch 1200, Loss: 0.362\n",
      "Epoch 11, Batch 1300, Loss: 0.298\n",
      "Epoch 11, Batch 1400, Loss: 0.271\n",
      "Epoch 11, Batch 1500, Loss: 0.379\n",
      "Epoch 11, Batch 1600, Loss: 0.279\n",
      "Epoch 11, Batch 1700, Loss: 0.276\n",
      "Epoch 11, Batch 1800, Loss: 0.336\n",
      "Epoch 12, Batch 100, Loss: 0.381\n",
      "Epoch 12, Batch 200, Loss: 0.270\n",
      "Epoch 12, Batch 300, Loss: 0.237\n",
      "Epoch 12, Batch 400, Loss: 0.298\n",
      "Epoch 12, Batch 500, Loss: 0.291\n",
      "Epoch 12, Batch 600, Loss: 0.207\n",
      "Epoch 12, Batch 700, Loss: 0.263\n",
      "Epoch 12, Batch 800, Loss: 0.256\n",
      "Epoch 12, Batch 900, Loss: 0.252\n",
      "Epoch 12, Batch 1000, Loss: 0.315\n",
      "Epoch 12, Batch 1100, Loss: 0.266\n",
      "Epoch 12, Batch 1200, Loss: 0.225\n",
      "Epoch 12, Batch 1300, Loss: 0.302\n",
      "Epoch 12, Batch 1400, Loss: 0.203\n",
      "Epoch 12, Batch 1500, Loss: 0.263\n",
      "Epoch 12, Batch 1600, Loss: 1.658\n",
      "Epoch 12, Batch 1700, Loss: 0.272\n",
      "Epoch 12, Batch 1800, Loss: 0.213\n",
      "Epoch 13, Batch 100, Loss: 0.281\n",
      "Epoch 13, Batch 200, Loss: 0.215\n",
      "Epoch 13, Batch 300, Loss: 2.146\n",
      "Epoch 13, Batch 400, Loss: 0.289\n",
      "Epoch 13, Batch 500, Loss: 0.333\n",
      "Epoch 13, Batch 600, Loss: 0.284\n",
      "Epoch 13, Batch 700, Loss: 0.237\n",
      "Epoch 13, Batch 800, Loss: 0.344\n",
      "Epoch 13, Batch 900, Loss: 0.274\n",
      "Epoch 13, Batch 1000, Loss: 0.189\n",
      "Epoch 13, Batch 1100, Loss: 4.130\n",
      "Epoch 13, Batch 1200, Loss: 0.279\n",
      "Epoch 13, Batch 1300, Loss: 0.310\n",
      "Epoch 13, Batch 1400, Loss: 0.264\n",
      "Epoch 13, Batch 1500, Loss: 0.211\n",
      "Epoch 13, Batch 1600, Loss: 3.432\n",
      "Epoch 13, Batch 1700, Loss: 0.332\n",
      "Epoch 13, Batch 1800, Loss: 0.265\n",
      "Epoch 14, Batch 100, Loss: 0.288\n",
      "Epoch 14, Batch 200, Loss: 0.299\n",
      "Epoch 14, Batch 300, Loss: 0.291\n",
      "Epoch 14, Batch 400, Loss: 0.311\n",
      "Epoch 14, Batch 500, Loss: 0.297\n",
      "Epoch 14, Batch 600, Loss: 0.228\n",
      "Epoch 14, Batch 700, Loss: 0.253\n",
      "Epoch 14, Batch 800, Loss: 0.233\n",
      "Epoch 14, Batch 900, Loss: 0.239\n",
      "Epoch 14, Batch 1000, Loss: 0.297\n",
      "Epoch 14, Batch 1100, Loss: 0.526\n",
      "Epoch 14, Batch 1200, Loss: 0.264\n",
      "Epoch 14, Batch 1300, Loss: 4.170\n",
      "Epoch 14, Batch 1400, Loss: 0.967\n",
      "Epoch 14, Batch 1500, Loss: 0.276\n",
      "Epoch 14, Batch 1600, Loss: 0.247\n",
      "Epoch 14, Batch 1700, Loss: 5.198\n",
      "Epoch 14, Batch 1800, Loss: 0.304\n",
      "Epoch 15, Batch 100, Loss: 0.275\n",
      "Epoch 15, Batch 200, Loss: 0.212\n",
      "Epoch 15, Batch 300, Loss: 0.260\n",
      "Epoch 15, Batch 400, Loss: 0.230\n",
      "Epoch 15, Batch 500, Loss: 0.186\n",
      "Epoch 15, Batch 600, Loss: 0.153\n",
      "Epoch 15, Batch 700, Loss: 15.811\n",
      "Epoch 15, Batch 800, Loss: 0.292\n",
      "Epoch 15, Batch 900, Loss: 0.140\n",
      "Epoch 15, Batch 1000, Loss: 1.640\n",
      "Epoch 15, Batch 1100, Loss: 0.211\n",
      "Epoch 15, Batch 1200, Loss: 4.183\n",
      "Epoch 15, Batch 1300, Loss: 0.178\n",
      "Epoch 15, Batch 1400, Loss: 0.173\n",
      "Epoch 15, Batch 1500, Loss: 0.193\n",
      "Epoch 15, Batch 1600, Loss: 0.233\n",
      "Epoch 15, Batch 1700, Loss: 0.130\n",
      "Epoch 15, Batch 1800, Loss: 3.068\n",
      "Epoch 16, Batch 100, Loss: 0.373\n",
      "Epoch 16, Batch 200, Loss: 0.345\n",
      "Epoch 16, Batch 300, Loss: 0.290\n",
      "Epoch 16, Batch 400, Loss: 0.302\n",
      "Epoch 16, Batch 500, Loss: 24.307\n",
      "Epoch 16, Batch 600, Loss: 0.280\n",
      "Epoch 16, Batch 700, Loss: 0.246\n",
      "Epoch 16, Batch 800, Loss: 0.304\n",
      "Epoch 16, Batch 900, Loss: 0.257\n",
      "Epoch 16, Batch 1000, Loss: 0.323\n",
      "Epoch 16, Batch 1100, Loss: 0.273\n",
      "Epoch 16, Batch 1200, Loss: 0.254\n",
      "Epoch 16, Batch 1300, Loss: 0.269\n",
      "Epoch 16, Batch 1400, Loss: 0.287\n",
      "Epoch 16, Batch 1500, Loss: 0.290\n",
      "Epoch 16, Batch 1600, Loss: 2.582\n",
      "Epoch 16, Batch 1700, Loss: 0.267\n",
      "Epoch 16, Batch 1800, Loss: 1.204\n",
      "Epoch 17, Batch 100, Loss: 0.287\n",
      "Epoch 17, Batch 200, Loss: 0.261\n",
      "Epoch 17, Batch 300, Loss: 0.193\n",
      "Epoch 17, Batch 400, Loss: 0.213\n",
      "Epoch 17, Batch 500, Loss: 0.340\n",
      "Epoch 17, Batch 600, Loss: 0.280\n",
      "Epoch 17, Batch 700, Loss: 2.913\n",
      "Epoch 17, Batch 800, Loss: 0.292\n",
      "Epoch 17, Batch 900, Loss: 0.308\n",
      "Epoch 17, Batch 1000, Loss: 0.314\n",
      "Epoch 17, Batch 1100, Loss: 0.269\n",
      "Epoch 17, Batch 1200, Loss: 0.287\n",
      "Epoch 17, Batch 1300, Loss: 0.232\n",
      "Epoch 17, Batch 1400, Loss: 0.238\n",
      "Epoch 17, Batch 1500, Loss: 0.272\n",
      "Epoch 17, Batch 1600, Loss: 0.303\n",
      "Epoch 17, Batch 1700, Loss: 0.268\n",
      "Epoch 17, Batch 1800, Loss: 0.240\n",
      "Epoch 18, Batch 100, Loss: 0.265\n",
      "Epoch 18, Batch 200, Loss: 0.183\n",
      "Epoch 18, Batch 300, Loss: 0.208\n",
      "Epoch 18, Batch 400, Loss: 0.213\n",
      "Epoch 18, Batch 500, Loss: 0.250\n",
      "Epoch 18, Batch 600, Loss: 0.286\n",
      "Epoch 18, Batch 700, Loss: 0.233\n",
      "Epoch 18, Batch 800, Loss: 0.331\n",
      "Epoch 18, Batch 900, Loss: 0.289\n",
      "Epoch 18, Batch 1000, Loss: 0.239\n",
      "Epoch 18, Batch 1100, Loss: 3.574\n",
      "Epoch 18, Batch 1200, Loss: 0.283\n",
      "Epoch 18, Batch 1300, Loss: 0.278\n",
      "Epoch 18, Batch 1400, Loss: 0.245\n",
      "Epoch 18, Batch 1500, Loss: 0.281\n",
      "Epoch 18, Batch 1600, Loss: 0.334\n",
      "Epoch 18, Batch 1700, Loss: 0.265\n",
      "Epoch 18, Batch 1800, Loss: 0.302\n",
      "Epoch 19, Batch 100, Loss: 0.276\n",
      "Epoch 19, Batch 200, Loss: 0.243\n",
      "Epoch 19, Batch 300, Loss: 0.226\n",
      "Epoch 19, Batch 400, Loss: 0.204\n",
      "Epoch 19, Batch 500, Loss: 0.221\n",
      "Epoch 19, Batch 600, Loss: 1.289\n",
      "Epoch 19, Batch 700, Loss: 0.287\n",
      "Epoch 19, Batch 800, Loss: 0.273\n",
      "Epoch 19, Batch 900, Loss: 0.337\n",
      "Epoch 19, Batch 1000, Loss: 1.132\n",
      "Epoch 19, Batch 1100, Loss: 0.269\n",
      "Epoch 19, Batch 1200, Loss: 0.292\n",
      "Epoch 19, Batch 1300, Loss: 0.290\n",
      "Epoch 19, Batch 1400, Loss: 0.280\n",
      "Epoch 19, Batch 1500, Loss: 0.276\n",
      "Epoch 19, Batch 1600, Loss: 0.316\n",
      "Epoch 19, Batch 1700, Loss: 0.266\n",
      "Epoch 19, Batch 1800, Loss: 0.301\n",
      "Epoch 20, Batch 100, Loss: 0.306\n",
      "Epoch 20, Batch 200, Loss: 0.280\n",
      "Epoch 20, Batch 300, Loss: 0.263\n",
      "Epoch 20, Batch 400, Loss: 0.250\n",
      "Epoch 20, Batch 500, Loss: 0.288\n",
      "Epoch 20, Batch 600, Loss: 0.299\n",
      "Epoch 20, Batch 700, Loss: 0.259\n",
      "Epoch 20, Batch 800, Loss: 0.250\n",
      "Epoch 20, Batch 900, Loss: 0.290\n",
      "Epoch 20, Batch 1000, Loss: 0.254\n",
      "Epoch 20, Batch 1100, Loss: 0.237\n",
      "Epoch 20, Batch 1200, Loss: 0.254\n",
      "Epoch 20, Batch 1300, Loss: 0.239\n",
      "Epoch 20, Batch 1400, Loss: 0.247\n",
      "Epoch 20, Batch 1500, Loss: 5.386\n",
      "Epoch 20, Batch 1600, Loss: 0.243\n",
      "Epoch 20, Batch 1700, Loss: 0.297\n",
      "Epoch 20, Batch 1800, Loss: 0.233\n",
      "Epoch 21, Batch 100, Loss: 0.238\n",
      "Epoch 21, Batch 200, Loss: 8.185\n",
      "Epoch 21, Batch 300, Loss: 0.262\n",
      "Epoch 21, Batch 400, Loss: 0.225\n",
      "Epoch 21, Batch 500, Loss: 0.225\n",
      "Epoch 21, Batch 600, Loss: 5.480\n",
      "Epoch 21, Batch 700, Loss: 0.280\n",
      "Epoch 21, Batch 800, Loss: 0.299\n",
      "Epoch 21, Batch 900, Loss: 0.276\n",
      "Epoch 21, Batch 1000, Loss: 0.310\n",
      "Epoch 21, Batch 1100, Loss: 0.312\n",
      "Epoch 21, Batch 1200, Loss: 0.334\n",
      "Epoch 21, Batch 1300, Loss: 0.232\n",
      "Epoch 21, Batch 1400, Loss: 0.310\n",
      "Epoch 21, Batch 1500, Loss: 0.303\n",
      "Epoch 21, Batch 1600, Loss: 0.290\n",
      "Epoch 21, Batch 1700, Loss: 0.303\n",
      "Epoch 21, Batch 1800, Loss: 0.312\n",
      "Epoch 22, Batch 100, Loss: 83.013\n",
      "Epoch 22, Batch 200, Loss: 0.212\n",
      "Epoch 22, Batch 300, Loss: 0.219\n",
      "Epoch 22, Batch 400, Loss: 0.128\n",
      "Epoch 22, Batch 500, Loss: 0.214\n",
      "Epoch 22, Batch 600, Loss: 0.136\n",
      "Epoch 22, Batch 700, Loss: 2.717\n",
      "Epoch 22, Batch 800, Loss: 0.234\n",
      "Epoch 22, Batch 900, Loss: 0.191\n",
      "Epoch 22, Batch 1000, Loss: 3.527\n",
      "Epoch 22, Batch 1100, Loss: 0.299\n",
      "Epoch 22, Batch 1200, Loss: 0.212\n",
      "Epoch 22, Batch 1300, Loss: 0.218\n",
      "Epoch 22, Batch 1400, Loss: 0.207\n",
      "Epoch 22, Batch 1500, Loss: 106.097\n",
      "Epoch 22, Batch 1600, Loss: 7.782\n",
      "Epoch 22, Batch 1700, Loss: 0.146\n",
      "Epoch 22, Batch 1800, Loss: 0.213\n",
      "Epoch 23, Batch 100, Loss: 0.187\n",
      "Epoch 23, Batch 200, Loss: 3.055\n",
      "Epoch 23, Batch 300, Loss: 0.174\n",
      "Epoch 23, Batch 400, Loss: 1.916\n",
      "Epoch 23, Batch 500, Loss: 0.253\n",
      "Epoch 23, Batch 600, Loss: 1.241\n",
      "Epoch 23, Batch 700, Loss: 0.180\n",
      "Epoch 23, Batch 800, Loss: 0.300\n",
      "Epoch 23, Batch 900, Loss: 0.340\n",
      "Epoch 23, Batch 1000, Loss: 0.945\n",
      "Epoch 23, Batch 1100, Loss: 0.186\n",
      "Epoch 23, Batch 1200, Loss: 0.203\n",
      "Epoch 23, Batch 1300, Loss: 0.184\n",
      "Epoch 23, Batch 1400, Loss: 0.144\n",
      "Epoch 23, Batch 1500, Loss: 1.179\n",
      "Epoch 23, Batch 1600, Loss: 0.196\n",
      "Epoch 23, Batch 1700, Loss: 0.185\n",
      "Epoch 23, Batch 1800, Loss: 0.193\n",
      "Epoch 24, Batch 100, Loss: 0.198\n",
      "Epoch 24, Batch 200, Loss: 0.172\n",
      "Epoch 24, Batch 300, Loss: 0.180\n",
      "Epoch 24, Batch 400, Loss: 0.159\n",
      "Epoch 24, Batch 500, Loss: 0.165\n",
      "Epoch 24, Batch 600, Loss: 0.699\n",
      "Epoch 24, Batch 700, Loss: 0.191\n",
      "Epoch 24, Batch 800, Loss: 0.188\n",
      "Epoch 24, Batch 900, Loss: 0.120\n",
      "Epoch 24, Batch 1000, Loss: 0.157\n",
      "Epoch 24, Batch 1100, Loss: 18.173\n",
      "Epoch 24, Batch 1200, Loss: 0.399\n",
      "Epoch 24, Batch 1300, Loss: 0.315\n",
      "Epoch 24, Batch 1400, Loss: 1.220\n",
      "Epoch 24, Batch 1500, Loss: 0.288\n",
      "Epoch 24, Batch 1600, Loss: 0.290\n",
      "Epoch 24, Batch 1700, Loss: 0.283\n",
      "Epoch 24, Batch 1800, Loss: 0.285\n",
      "Epoch 25, Batch 100, Loss: 0.312\n",
      "Epoch 25, Batch 200, Loss: 0.241\n",
      "Epoch 25, Batch 300, Loss: 0.269\n",
      "Epoch 25, Batch 400, Loss: 0.321\n",
      "Epoch 25, Batch 500, Loss: 0.306\n",
      "Epoch 25, Batch 600, Loss: 0.229\n",
      "Epoch 25, Batch 700, Loss: 0.299\n",
      "Epoch 25, Batch 800, Loss: 0.242\n",
      "Epoch 25, Batch 900, Loss: 0.237\n",
      "Epoch 25, Batch 1000, Loss: 0.219\n",
      "Epoch 25, Batch 1100, Loss: 0.443\n",
      "Epoch 25, Batch 1200, Loss: 0.241\n",
      "Epoch 25, Batch 1300, Loss: 0.230\n",
      "Epoch 25, Batch 1400, Loss: 1.495\n",
      "Epoch 25, Batch 1500, Loss: 0.309\n",
      "Epoch 25, Batch 1600, Loss: 0.292\n",
      "Epoch 25, Batch 1700, Loss: 0.333\n",
      "Epoch 25, Batch 1800, Loss: 0.284\n",
      "Done. Total Training Time: 70.09s\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "num_epochs = 25\n",
    "training_losses = [] \n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0  # Track the loss for each epoch\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # 1. get a batch of data, move to proper device\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 2. reset gradients (aside from first iteration, these have some value)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 3. run the data through the model, get predictions. forward pass\n",
    "        y_pred = model(inputs)\n",
    "\n",
    "        # 4. Compute the loss\n",
    "        loss = loss_func(y_pred, labels)\n",
    "\n",
    "        # 5. Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # 6. update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        training_losses.append(loss.item())\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "end_time = time.time() \n",
    "total_training_time = end_time - start_time  # Calculate total training time\n",
    "\n",
    "print(f'Done. Total Training Time: {total_training_time:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b84c6269-739a-48ca-b5af-7982a9ac598f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAHHCAYAAACiOWx7AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAARiBJREFUeJzt3XlcVmX+//H3DXgDLoArSJE6Wi5pVlpGiznJSOU0Ws6UZmXmZE04kzmt30mz1S3NNNNWtcay5ZdW7uSaihuKCxpaLpAKpMjixnZfvz+MM9yCekDwvpHX8/G4Hw/uc677nM85B7jf93Wuc26HMcYIAAAAZ+Xj6QIAAACqAkITAACADYQmAAAAGwhNAAAANhCaAAAAbCA0AQAA2EBoAgAAsIHQBAAAYAOhCQAAwAZCE4Aq6eGHH1bTpk3L9drhw4fL4XBUbEEALnqEJgAVyuFw2HosW7bM06V6xMMPP6zatWt7ugwA5eDgu+cAVKT//ve/bs8/+eQTxcbG6tNPP3Wb/qc//UmhoaHlXk9+fr5cLpf8/f3L/NqCggIVFBQoICCg3Osvr4cfflhff/21jh49esHXDeD8+Hm6AAAXlwceeMDt+Zo1axQbG1ti+umOHz+umjVr2l5PjRo1ylWfJPn5+cnPj39/AMqG03MALrguXbqobdu2io+PV+fOnVWzZk393//9nyTp22+/Vffu3RUeHi5/f381b95cr776qgoLC92WcfqYpr1798rhcOjNN9/U+++/r+bNm8vf31/XXXed1q9f7/ba0sY0ORwODRo0SLNnz1bbtm3l7++vK6+8UgsWLChR/7Jly9SxY0cFBASoefPmeu+99yp8nNRXX32lDh06KDAwUA0aNNADDzyg/fv3u7VJTU1V//79demll8rf31+NGzdWjx49tHfvXqvNhg0bFB0drQYNGigwMFDNmjXTI488UmF1AtUJH7UAeMThw4d1xx13qHfv3nrggQesU3XTpk1T7dq1NWTIENWuXVtLlizRsGHDlJ2drTFjxpxzuZ999plycnL02GOPyeFwaPTo0brnnnu0e/fuc/ZOrVy5Ut98842eeOIJ1alTRxMmTFCvXr2UnJys+vXrS5I2bdqk22+/XY0bN9bLL7+swsJCvfLKK2rYsOH575TfTZs2Tf3799d1112nESNGKC0tTW+//bZWrVqlTZs2KSQkRJLUq1cvJSYm6p///KeaNm2q9PR0xcbGKjk52XrerVs3NWzYUM8//7xCQkK0d+9effPNNxVWK1CtGACoRDExMeb0fzW33nqrkWSmTJlSov3x48dLTHvsscdMzZo1zcmTJ61p/fr1M02aNLGe79mzx0gy9evXNxkZGdb0b7/91kgy33//vTXtpZdeKlGTJON0Os3PP/9sTdu8ebORZCZOnGhNu+uuu0zNmjXN/v37rWm7du0yfn5+JZZZmn79+platWqdcX5eXp5p1KiRadu2rTlx4oQ1fc6cOUaSGTZsmDHGmCNHjhhJZsyYMWdc1qxZs4wks379+nPWBeDcOD0HwCP8/f3Vv3//EtMDAwOtn3NycnTo0CHdcsstOn78uH766adzLve+++5T3bp1ree33HKLJGn37t3nfG1UVJSaN29uPb/qqqsUFBRkvbawsFA//PCDevbsqfDwcKtdixYtdMcdd5xz+XZs2LBB6enpeuKJJ9wGqnfv3l2tWrXS3LlzJZ3aT06nU8uWLdORI0dKXVZRj9ScOXOUn59fIfUB1RmhCYBHXHLJJXI6nSWmJyYm6u6771ZwcLCCgoLUsGFDaxB5VlbWOZd72WWXuT0vClBnChZne23R64tem56erhMnTqhFixYl2pU2rTz27dsnSWrZsmWJea1atbLm+/v7a9SoUZo/f75CQ0PVuXNnjR49WqmpqVb7W2+9Vb169dLLL7+sBg0aqEePHpo6dapyc3MrpFaguiE0AfCI4j1KRTIzM3Xrrbdq8+bNeuWVV/T9998rNjZWo0aNkiS5XK5zLtfX17fU6cbG3VXO57WeMHjwYO3cuVMjRoxQQECAhg4dqtatW2vTpk2STg1u//rrrxUXF6dBgwZp//79euSRR9ShQwdueQCUA6EJgNdYtmyZDh8+rGnTpunJJ5/Un//8Z0VFRbmdbvOkRo0aKSAgQD///HOJeaVNK48mTZpIkpKSkkrMS0pKsuYXad68uf79739r0aJF2rZtm/Ly8jR27Fi3NjfccINef/11bdiwQTNmzFBiYqJmzpxZIfUC1QmhCYDXKOrpKd6zk5eXp3fffddTJbnx9fVVVFSUZs+erQMHDljTf/75Z82fP79C1tGxY0c1atRIU6ZMcTuNNn/+fO3YsUPdu3eXdOq+VidPnnR7bfPmzVWnTh3rdUeOHCnRS3b11VdLEqfogHLglgMAvMaNN96ounXrql+/fvrXv/4lh8OhTz/91KtOjw0fPlyLFi3STTfdpH/84x8qLCzUO++8o7Zt2yohIcHWMvLz8/Xaa6+VmF6vXj098cQTGjVqlPr3769bb71Vffr0sW450LRpUz311FOSpJ07d6pr166699571aZNG/n5+WnWrFlKS0tT7969JUnTp0/Xu+++q7vvvlvNmzdXTk6OPvjgAwUFBenOO++ssH0CVBeEJgBeo379+pozZ47+/e9/68UXX1TdunX1wAMPqGvXroqOjvZ0eZKkDh06aP78+Xr66ac1dOhQRURE6JVXXtGOHTtsXd0nneo9Gzp0aInpzZs31xNPPKGHH35YNWvW1MiRI/Xcc8+pVq1auvvuuzVq1CjririIiAj16dNHixcv1qeffio/Pz+1atVKX375pXr16iXp1EDwdevWaebMmUpLS1NwcLCuv/56zZgxQ82aNauwfQJUF3z3HABUgJ49eyoxMVG7du3ydCkAKgljmgCgjE6cOOH2fNeuXZo3b566dOnimYIAXBD0NAFAGTVu3FgPP/yw/vCHP2jfvn2aPHmycnNztWnTJl1++eWeLg9AJWFMEwCU0e23367PP/9cqamp8vf3V2RkpN544w0CE3CRo6cJAADABsY0AQAA2EBoAgAAsIExTRXE5XLpwIEDqlOnjhwOh6fLAQAANhhjlJOTo/DwcPn4nL0vidBUQQ4cOKCIiAhPlwEAAMohJSVFl1566VnbEJoqSJ06dSSd2ulBQUEergYAANiRnZ2tiIgI6338bAhNFaTolFxQUBChCQCAKsbO0BoGggMAANhAaAIAALCB0AQAAGADoQkAAMAGQhMAAIANhCYAAAAbCE0AAAA2EJoAAABsIDQBAADYQGgCAACwgdAEAABgA6EJAADABkITAABV2Mn8Qk+XUG0QmgAAqKImLN6lVkMXaGlSuqdLqRYITQAAVFHjYndKkobO3ubhSqoHQhMAAIANhCYAAAAbCE0AAAA2EJoAAABsIDQBAADYQGgCAACwgdAEAABgA6EJAADABkITAACADYQmAAAAGwhNAAAANhCaAAAAbCA0AQAA2EBoAgAAsMGjoWnFihW66667FB4eLofDodmzZ7vNN8Zo2LBhaty4sQIDAxUVFaVdu3a5tcnIyFDfvn0VFBSkkJAQDRgwQEePHnVrs2XLFt1yyy0KCAhQRESERo8eXaKWr776Sq1atVJAQIDatWunefPmVfj2AgCAqsujoenYsWNq3769Jk2aVOr80aNHa8KECZoyZYrWrl2rWrVqKTo6WidPnrTa9O3bV4mJiYqNjdWcOXO0YsUKDRw40JqfnZ2tbt26qUmTJoqPj9eYMWM0fPhwvf/++1ab1atXq0+fPhowYIA2bdqknj17qmfPntq2bVvlbTwAAKhajJeQZGbNmmU9d7lcJiwszIwZM8aalpmZafz9/c3nn39ujDFm+/btRpJZv3691Wb+/PnG4XCY/fv3G2OMeffdd03dunVNbm6u1ea5554zLVu2tJ7fe++9pnv37m71dOrUyTz22GO268/KyjKSTFZWlu3XAABwPpo8N8c0eW6OuWnkYk+XUmWV5f3ba8c07dmzR6mpqYqKirKmBQcHq1OnToqLi5MkxcXFKSQkRB07drTaREVFycfHR2vXrrXadO7cWU6n02oTHR2tpKQkHTlyxGpTfD1FbYrWU5rc3FxlZ2e7PQAAwMXLa0NTamqqJCk0NNRtemhoqDUvNTVVjRo1cpvv5+enevXqubUpbRnF13GmNkXzSzNixAgFBwdbj4iIiLJuIgAAqEK8NjR5uxdeeEFZWVnWIyUlxdMlAQCASuS1oSksLEySlJaW5jY9LS3NmhcWFqb09HS3+QUFBcrIyHBrU9oyiq/jTG2K5pfG399fQUFBbg8AAHDx8trQ1KxZM4WFhWnx4sXWtOzsbK1du1aRkZGSpMjISGVmZio+Pt5qs2TJErlcLnXq1Mlqs2LFCuXn51ttYmNj1bJlS9WtW9dqU3w9RW2K1gMAAODR0HT06FElJCQoISFB0qnB3wkJCUpOTpbD4dDgwYP12muv6bvvvtPWrVv10EMPKTw8XD179pQktW7dWrfffrseffRRrVu3TqtWrdKgQYPUu3dvhYeHS5Luv/9+OZ1ODRgwQImJifriiy/09ttva8iQIVYdTz75pBYsWKCxY8fqp59+0vDhw7VhwwYNGjToQu8SAADgrS7A1XxntHTpUiOpxKNfv37GmFO3HRg6dKgJDQ01/v7+pmvXriYpKcltGYcPHzZ9+vQxtWvXNkFBQaZ///4mJyfHrc3mzZvNzTffbPz9/c0ll1xiRo4cWaKWL7/80lxxxRXG6XSaK6+80sydO7dM28ItBwAAFxq3HDh/ZXn/dhhjjAcz20UjOztbwcHBysrKYnwTAOCCaPr8XEnSpXUDtfK52zxcTdVUlvdvrx3TBAAA4E0ITQAAADYQmgAAAGwgNAEAANhAaAIAALCB0AQAAGADoQkAAMAGQhMAAIANhCYAAAAbCE0AAAA2EJoAAABsIDQBAADYQGgCAACwgdAEAABgA6EJAADABkITAACADYQmAAAAGwhNAAAANhCaAAAAbCA0AQAA2EBoAgAAsIHQBAAAYAOhCQAAwAZCEwAAgA2EJgAAABsITQAAADYQmgAAAGwgNAEAANhAaAIAALCB0AQAAGADoQkAAMAGQhMAAIANhCYAAAAbCE0AAAA2EJoAAABsIDQBAADYQGgCAACwgdAEAABgA6EJAADABkITAACADYQmAAAAGwhNAAAANhCaAAAAbCA0AQAA2EBoAgAAsIHQBAAAYAOhCQAAwAZCEwAAgA2EJgAAABsITQAAADYQmgAAAGwgNAEAANjg1aGpsLBQQ4cOVbNmzRQYGKjmzZvr1VdflTHGamOM0bBhw9S4cWMFBgYqKipKu3btcltORkaG+vbtq6CgIIWEhGjAgAE6evSoW5stW7bolltuUUBAgCIiIjR69OgLso0AAKBq8OrQNGrUKE2ePFnvvPOOduzYoVGjRmn06NGaOHGi1Wb06NGaMGGCpkyZorVr16pWrVqKjo7WyZMnrTZ9+/ZVYmKiYmNjNWfOHK1YsUIDBw605mdnZ6tbt25q0qSJ4uPjNWbMGA0fPlzvv//+Bd1eAADgvfw8XcDZrF69Wj169FD37t0lSU2bNtXnn3+udevWSTrVyzR+/Hi9+OKL6tGjhyTpk08+UWhoqGbPnq3evXtrx44dWrBggdavX6+OHTtKkiZOnKg777xTb775psLDwzVjxgzl5eXp448/ltPp1JVXXqmEhASNGzfOLVwBAIDqy6t7mm688UYtXrxYO3fulCRt3rxZK1eu1B133CFJ2rNnj1JTUxUVFWW9Jjg4WJ06dVJcXJwkKS4uTiEhIVZgkqSoqCj5+Pho7dq1VpvOnTvL6XRabaKjo5WUlKQjR46UWltubq6ys7PdHgAAlEduQaFiZmzUlxtSPF0KzsKre5qef/55ZWdnq1WrVvL19VVhYaFef/119e3bV5KUmpoqSQoNDXV7XWhoqDUvNTVVjRo1cpvv5+enevXqubVp1qxZiWUUzatbt26J2kaMGKGXX365ArYSAFDdfb42WXO3HtTcrQd1b8cIT5eDM/DqnqYvv/xSM2bM0GeffaaNGzdq+vTpevPNNzV9+nRPl6YXXnhBWVlZ1iMlhU8HAIDyyTyR7+kSYINX9zQ988wzev7559W7d29JUrt27bRv3z6NGDFC/fr1U1hYmCQpLS1NjRs3tl6Xlpamq6++WpIUFham9PR0t+UWFBQoIyPDen1YWJjS0tLc2hQ9L2pzOn9/f/n7+5//RgIAgCrBq3uajh8/Lh8f9xJ9fX3lcrkkSc2aNVNYWJgWL15szc/OztbatWsVGRkpSYqMjFRmZqbi4+OtNkuWLJHL5VKnTp2sNitWrFB+/v+SfmxsrFq2bFnqqTkAAFD9eHVouuuuu/T6669r7ty52rt3r2bNmqVx48bp7rvvliQ5HA4NHjxYr732mr777jtt3bpVDz30kMLDw9WzZ09JUuvWrXX77bfr0Ucf1bp167Rq1SoNGjRIvXv3Vnh4uCTp/vvvl9Pp1IABA5SYmKgvvvhCb7/9toYMGeKpTQcAAF7Gq0/PTZw4UUOHDtUTTzyh9PR0hYeH67HHHtOwYcOsNs8++6yOHTumgQMHKjMzUzfffLMWLFiggIAAq82MGTM0aNAgde3aVT4+PurVq5cmTJhgzQ8ODtaiRYsUExOjDh06qEGDBho2bBi3GwAAABaHKX57bZRbdna2goODlZWVpaCgIE+XAwCoQsb/sFPjfzj1bRZ7R3a3/bqmz8+VJF1aN1Arn7utUmq72JXl/durT88BAAB4C0ITAACADYQmAAAAGwhNAAAANhCaAAAAbCA0AQDgYQ45PF0CbCA0AQAA2EBoAgAAsIHQBAAAYAOhCQAAwAZCEwAAgA2EJgAAABsITQAAADYQmgAAAGwgNAEAANhAaAIAALCB0AQAgIc5+BaVKoHQBAAAYAOhCQAAwAZCEwAAgA2EJgAAABsITQAAADYQmgAAAGwgNAEAUMVxy4ILg9AEAABgA6EJAADABkITAACADYQmAAA8jCFJVQOhCQAAwAZCEwAAgA2EJgAAABsITQAAADYQmgAAAGwgNAEAANhAaAIAALCB0AQAAGADoQkAAMAGQhMAAIANhCYAADzMwfeoVAmEJgAAABsITQAAADYQmgAAAGwgNAEAANhAaAIAALCB0AQAAGADoQkAAMAGQhMAAIANhCYAAAAbCE0AAAA2EJoAAPAwB9+jUiUQmgAAAGwgNAEAANjg9aFp//79euCBB1S/fn0FBgaqXbt22rBhgzXfGKNhw4apcePGCgwMVFRUlHbt2uW2jIyMDPXt21dBQUEKCQnRgAEDdPToUbc2W7Zs0S233KKAgABFRERo9OjRF2T7AABA1eDVoenIkSO66aabVKNGDc2fP1/bt2/X2LFjVbduXavN6NGjNWHCBE2ZMkVr165VrVq1FB0drZMnT1pt+vbtq8TERMXGxmrOnDlasWKFBg4caM3Pzs5Wt27d1KRJE8XHx2vMmDEaPny43n///Qu6vQAAwHv5ebqAsxk1apQiIiI0depUa1qzZs2sn40xGj9+vF588UX16NFDkvTJJ58oNDRUs2fPVu/evbVjxw4tWLBA69evV8eOHSVJEydO1J133qk333xT4eHhmjFjhvLy8vTxxx/L6XTqyiuvVEJCgsaNG+cWrgAAQPXl1T1N3333nTp27Ki//e1vatSoka655hp98MEH1vw9e/YoNTVVUVFR1rTg4GB16tRJcXFxkqS4uDiFhIRYgUmSoqKi5OPjo7Vr11ptOnfuLKfTabWJjo5WUlKSjhw5UtmbCQAAqgCvDk27d+/W5MmTdfnll2vhwoX6xz/+oX/961+aPn26JCk1NVWSFBoa6va60NBQa15qaqoaNWrkNt/Pz0/16tVza1PaMoqv43S5ubnKzs52ewAAgIuXV5+ec7lc6tixo9544w1J0jXXXKNt27ZpypQp6tevn0drGzFihF5++WWP1gAAAC4cr+5paty4sdq0aeM2rXXr1kpOTpYkhYWFSZLS0tLc2qSlpVnzwsLClJ6e7ja/oKBAGRkZbm1KW0bxdZzuhRdeUFZWlvVISUkpzyYCAIAqwqtD00033aSkpCS3aTt37lSTJk0knRoUHhYWpsWLF1vzs7OztXbtWkVGRkqSIiMjlZmZqfj4eKvNkiVL5HK51KlTJ6vNihUrlJ+fb7WJjY1Vy5Yt3a7UK87f319BQUFuDwAAcPHy6tD01FNPac2aNXrjjTf0888/67PPPtP777+vmJgYSaduOz948GC99tpr+u6777R161Y99NBDCg8PV8+ePSWd6pm6/fbb9eijj2rdunVatWqVBg0apN69eys8PFySdP/998vpdGrAgAFKTEzUF198obfffltDhgzx1KYDAAAv49Vjmq677jrNmjVLL7zwgl555RU1a9ZM48ePV9++fa02zz77rI4dO6aBAwcqMzNTN998sxYsWKCAgACrzYwZMzRo0CB17dpVPj4+6tWrlyZMmGDNDw4O1qJFixQTE6MOHTqoQYMGGjZsGLcbAAAAFocxxni6iItBdna2goODlZWVxak6AECZTFr6s8YsPDUcZe/I7rZf1/T5uZKkiHqB+vHZ2yqltotdWd6/vfr0HAAAgLcgNAEAANhAaAIAALCB0AQAAGBDuUJTSkqKfv31V+v5unXrNHjwYL3//vsVVhgAAIA3KVdouv/++7V06VJJp76b7U9/+pPWrVun//znP3rllVcqtEAAAABvUK7QtG3bNl1//fWSpC+//FJt27bV6tWrNWPGDE2bNq0i6wMAAPAK5QpN+fn58vf3lyT98MMP+stf/iJJatWqlQ4ePFhx1QEAAHiJcoWmK6+8UlOmTNGPP/6o2NhY3X777ZKkAwcOqH79+hVaIAAAFzuHw9MVwI5yhaZRo0bpvffeU5cuXdSnTx+1b99ekvTdd99Zp+0AAAAuJuX67rkuXbro0KFDys7OVt26da3pAwcOVM2aNSusOAAAAG9Rrp6mEydOKDc31wpM+/bt0/jx45WUlKRGjRpVaIEAAADeoFyhqUePHvrkk08kSZmZmerUqZPGjh2rnj17avLkyRVaIAAAgDcoV2jauHGjbrnlFknS119/rdDQUO3bt0+ffPKJJkyYUKEFAgAAeINyhabjx4+rTp06kqRFixbpnnvukY+Pj2644Qbt27evQgsEAADwBuUKTS1atNDs2bOVkpKihQsXqlu3bpKk9PR0BQUFVWiBAADg7IzxdAXVQ7lC07Bhw/T000+radOmuv766xUZGSnpVK/TNddcU6EFAgAAeINy3XLgr3/9q26++WYdPHjQukeTJHXt2lV33313hRUHAADgLcoVmiQpLCxMYWFh+vXXXyVJl156KTe2BAAAF61ynZ5zuVx65ZVXFBwcrCZNmqhJkyYKCQnRq6++KpfLVdE1AgBwUXOI71GpCsrV0/Sf//xHH330kUaOHKmbbrpJkrRy5UoNHz5cJ0+e1Ouvv16hRQIAAHhauULT9OnT9eGHH+ovf/mLNe2qq67SJZdcoieeeILQBAAALjrlOj2XkZGhVq1alZjeqlUrZWRknHdRAAAA3qZcoal9+/Z65513Skx/5513dNVVV513UQAAAN6mXKfnRo8ere7du+uHH36w7tEUFxenlJQUzZs3r0ILBAAA8Abl6mm69dZbtXPnTt19993KzMxUZmam7rnnHiUmJurTTz+t6BoBAAA8rtz3aQoPDy8x4Hvz5s366KOP9P777593YQAAAN6kXD1NAAAA1Q2hCQAAwAZCEwAAgA1lGtN0zz33nHV+Zmbm+dQCAEC15OBbVKqEMoWm4ODgc85/6KGHzqsgAAAAb1Sm0DR16tTKqgMAAMCrMaYJAADABkITAACADYQmAAAAGwhNAAAANhCaAAAAbCA0AQAA2EBoAgAAsIHQBAAAYAOhCQAAD+NbVKoGQhMAAIANhCYAAAAbCE0AAAA2EJoAAABsIDQBAADYQGgCAACwgdAEAABgA6EJAADABkITAACADYQmAAAAGwhNAAB4mIPvUakSqlRoGjlypBwOhwYPHmxNO3nypGJiYlS/fn3Vrl1bvXr1UlpamtvrkpOT1b17d9WsWVONGjXSM888o4KCArc2y5Yt07XXXit/f3+1aNFC06ZNuwBbBAAAqooqE5rWr1+v9957T1dddZXb9Keeekrff/+9vvrqKy1fvlwHDhzQPffcY80vLCxU9+7dlZeXp9WrV2v69OmaNm2ahg0bZrXZs2ePunfvrj/+8Y9KSEjQ4MGD9fe//10LFy68YNsHAAC8W5UITUePHlXfvn31wQcfqG7dutb0rKwsffTRRxo3bpxuu+02dejQQVOnTtXq1au1Zs0aSdKiRYu0fft2/fe//9XVV1+tO+64Q6+++qomTZqkvLw8SdKUKVPUrFkzjR07Vq1bt9agQYP017/+VW+99ZZHthcAAHifKhGaYmJi1L17d0VFRblNj4+PV35+vtv0Vq1a6bLLLlNcXJwkKS4uTu3atVNoaKjVJjo6WtnZ2UpMTLTanL7s6Ohoaxmlyc3NVXZ2ttsDAABcvPw8XcC5zJw5Uxs3btT69etLzEtNTZXT6VRISIjb9NDQUKWmplptigemovlF887WJjs7WydOnFBgYGCJdY8YMUIvv/xyubcLAIAixni6Atjh1T1NKSkpevLJJzVjxgwFBAR4uhw3L7zwgrKysqxHSkqKp0sCAACVyKtDU3x8vNLT03XttdfKz89Pfn5+Wr58uSZMmCA/Pz+FhoYqLy9PmZmZbq9LS0tTWFiYJCksLKzE1XRFz8/VJigoqNReJkny9/dXUFCQ2wMAAFy8vDo0de3aVVu3blVCQoL16Nixo/r27Wv9XKNGDS1evNh6TVJSkpKTkxUZGSlJioyM1NatW5Wenm61iY2NVVBQkNq0aWO1Kb6MojZFywAAAPDqMU116tRR27Zt3abVqlVL9evXt6YPGDBAQ4YMUb169RQUFKR//vOfioyM1A033CBJ6tatm9q0aaMHH3xQo0ePVmpqql588UXFxMTI399fkvT444/rnXfe0bPPPqtHHnlES5Ys0Zdffqm5c+de2A0GAABey6tDkx1vvfWWfHx81KtXL+Xm5io6OlrvvvuuNd/X11dz5szRP/7xD0VGRqpWrVrq16+fXnnlFatNs2bNNHfuXD311FN6++23demll+rDDz9UdHS0JzYJAAB4oSoXmpYtW+b2PCAgQJMmTdKkSZPO+JomTZpo3rx5Z11uly5dtGnTpoooEQCAMuFrVKoGrx7TBAAA4C0ITQAAADYQmgAAAGwgNAEAANhAaAIAALCB0AQAAGADoQkAAMAGQhMAAIANhCYAAKo4bo55YRCaAAAAbCA0AQDgYQ7RVVQVEJoAAABsIDQBAADYQGgCAACwgdAEAABgA6EJAADABkITAACADYQmAAAAGwhNAAAANhCaAAAAbCA0AQAA2EBoAgDAw/jC3aqB0AQAAGADoQkAAMAGQhMAAIANhCYAAAAbCE0AAAA2EJoAAABsIDQBAADYQGgCAACwgdAEAABgA6EJAADABkITAACADYQmAAAAGwhNAAAANhCaAAAAbCA0AQAA2EBoAgAAsIHQBAAAYAOhCQAAwAZCEwAAgA2EJgAAABsITQBQTaRkHFfW8XxPlwFUWX6eLgAAUPlSs07qltFLJUl7R3b3cDU4ncPh8HQJsIGeJgCoBhJSMj1dAlDlEZoAAABsIDQBAADYQGgCAACwgdAEAABgA6EJAKoBLs4Czh+hCQAAwAZCEwAAgA1eHZpGjBih6667TnXq1FGjRo3Us2dPJSUlubU5efKkYmJiVL9+fdWuXVu9evVSWlqaW5vk5GR1795dNWvWVKNGjfTMM8+ooKDArc2yZct07bXXyt/fXy1atNC0adMqe/MAAEAV4tWhafny5YqJidGaNWsUGxur/Px8devWTceOHbPaPPXUU/r+++/11Vdfafny5Tpw4IDuuecea35hYaG6d++uvLw8rV69WtOnT9e0adM0bNgwq82ePXvUvXt3/fGPf1RCQoIGDx6sv//971q4cOEF3V4AAOC9vPprVBYsWOD2fNq0aWrUqJHi4+PVuXNnZWVl6aOPPtJnn32m2267TZI0depUtW7dWmvWrNENN9ygRYsWafv27frhhx8UGhqqq6++Wq+++qqee+45DR8+XE6nU1OmTFGzZs00duxYSVLr1q21cuVKvfXWW4qOjr7g2w0AFY1x4N6N41M1eHVP0+mysrIkSfXq1ZMkxcfHKz8/X1FRUVabVq1a6bLLLlNcXJwkKS4uTu3atVNoaKjVJjo6WtnZ2UpMTLTaFF9GUZuiZQAAAHh1T1NxLpdLgwcP1k033aS2bdtKklJTU+V0OhUSEuLWNjQ0VKmpqVab4oGpaH7RvLO1yc7O1okTJxQYGFiintzcXOXm5lrPs7Ozz28DAQCAV6syPU0xMTHatm2bZs6c6elSJJ0apB4cHGw9IiIiPF0SAACoRFUiNA0aNEhz5szR0qVLdemll1rTw8LClJeXp8zMTLf2aWlpCgsLs9qcfjVd0fNztQkKCiq1l0mSXnjhBWVlZVmPlJSU89pGAADg3bw6NBljNGjQIM2aNUtLlixRs2bN3OZ36NBBNWrU0OLFi61pSUlJSk5OVmRkpCQpMjJSW7duVXp6utUmNjZWQUFBatOmjdWm+DKK2hQtozT+/v4KCgpyewCAt3JwS3DgvHn1mKaYmBh99tln+vbbb1WnTh1rDFJwcLACAwMVHBysAQMGaMiQIapXr56CgoL0z3/+U5GRkbrhhhskSd26dVObNm304IMPavTo0UpNTdWLL76omJgY+fv7S5Ief/xxvfPOO3r22Wf1yCOPaMmSJfryyy81d+5cj207AADwLl7d0zR58mRlZWWpS5cuaty4sfX44osvrDZvvfWW/vznP6tXr17q3LmzwsLC9M0331jzfX19NWfOHPn6+ioyMlIPPPCAHnroIb3yyitWm2bNmmnu3LmKjY1V+/btNXbsWH344YfcbgAAAFi8uqfJGHPONgEBAZo0aZImTZp0xjZNmjTRvHnzzrqcLl26aNOmTWWuEQAAVA9e3dMEAADgLQhNAAAANhCaAKAa4No578bFjVUDoQkAAMAGQhMAAIANhCYAqAbOfS0ygHMhNAEAgHJbmJiq/8zaqvxCl6dLqXRefZ8mAEDFYJwxKstjn8ZLklqF1dGDkU09W0wlo6cJAACct/ScXE+XUOkITQAAADYQmgAAAGwgNAFAFfDLb0c1aenPOpZb4OlSgGqLgeAAUAV0HbtckpSefVIv92hb5tdzx2ng/NHTBABVSHzyEU+XgEpApq0aCE1ANTUudqduG7tMmcfzPF0KAFQJhCagmpqweJd2/3ZMH6/c4+lSAKBKIDQB1Vyh4Qs2AMAOQhMAVAMMBAfOH6EJAADABkITAACADYQmAAAAGwhNwO9+Tj+qbfuzPF0GcFaM2wc8hzuCA7+LGnfqjsubhv5JdWs5PVwNULEc3D4ROG/0NAGnScs56ekSAKDKqQ69oIQmAAA8zME9IaoEQhNwmurwaQkAUHaEJgAAABsITQBQHXD2BzhvhCbgNJyegzdj6AvgOYQmAAAAGwhNQDXH/XuqlnL3hNKDCpw3QhPgAZuSj2hT8hFPlwGgAuz+7ai6vbVc3ybs93QpqGSEJuA0ppI/kp/ML9Td767W3e+u1om8wkpdF2ChQ7HSPPv1Fu1MO6onZyZ4uhRUMkITcIEdyy343895BWdpCaAqOM6HH0nV4yIFQhNwgRW/8y9X6gFVX3UICziF0AScprKDTPH/r5V9KtAOb6gBqMoqIjQRvKoGQhNwgTncUxOAKs4brkBNyTihQhf/UCoboQm4wIr/g+VfHC4Uz7+tX7y8pZdo7KIkT5dw0SM04aKQX+jS0dwqMqi62D9YxjQBVZ+XZCa9u+wXT5dw0SM04aLQZcwytX1poTKP53m6lHMq/qmU8USlS88+KUOiLBW7xfPyClzuE7ylqwmVjtCEi8L+zBOSpA17vf+Gkfx7PbsF2w7q+jcW6+mvtni6FKCEbfuzdMWL8zVqwU/WNP6mT6kOgZ7QBEh66ouEC7Ke33JylXHsf71h1eGfzLkkHz6uz9YmW5/ex/+wS5L0/zb+6smygFKNnH8qLE3mVFi15OfpAgBPMcZY90yatanyv/7gZH6hrnv9B/caKn2t3u/WN5fKGCnjWK4G3Xa5p8u5aDk4hVRp2LXVBz1NqJZSMo6r0xuL9e6yny/YOov3MBXxhnE7nr5cumgXxO0+7NE6zuV4XoHW782Qq5Iu6y50GZ3MP/edpYu/Qe87fEyvz92utOyTlVITSiotIJGZqg9CE2z7fvMBvbNkl6fLqBAjF/yk9JxcjV5Q8hJduznGGKOsE/nnVYcXZKZKc+RYngZ+skGLElNttXf9PrbWW3tEHvhwrf42JU7TVu+tkOUZYzT8u0TrNM9f3lmp1sMWKOek/d+pXpPj9MGPe/TEjI0VUhPKx+e039ldaTl66osE7Tl0zEMVobIQmmDbPz/fpDcX7dTGZO8fbH0uFdHD8/h/49X+5UVKPJBVARVdfMYsStKi7Wka+Gm8rfYuL0+QG5MzJUlfbkipkOUlpeVo2uq91oDixAPZMkZatyfD9jIOHc2VJMXvq/p/k8X9sD1NH/6429NllKq0UH/6pL+9F6dZm/brgQ/XVvj61+3J0KgFPym3gO+78wRCE8os46j3XtZvt5OiIk5JLUxMkyRNW7XX3jpLWaUnckKhy7h9aXBlOZSTW6b2Xp6ZLBUV7k6c4Uteuamz9PdPNui1uTu0Ya/9AOlJp/8/yTx+qrew6Kpee8uw59734jR52S+aeo7/O+k5J7Vg20EVFLrO2g5lQ2iC19tz6JiGf5eog1n2/wGdzcpdhzR368Ezzi/rvZPsvsmV9l7rifs03TN5ta58aeFZ2yxMTNVCm6fVzuT0UxbnUlXuWVXZoaayety886Tn2R3MqiJjtTywc/ee49Rf9Fsr9Ph/N+qTuH0XqKLqgdAEr/fXyas1bfVePfZpvI7nnb2HxM77zQMfVWyX+blq8jabUzLPOv9YboEe+zRej30ar3GLknTPu6vK1TPl61PG0PT7sfP2N/eKCjVnWsq5Th1fiB65rOP5WrHztwr9LrO4Xw5rzpYDZXqNN56yLe330xt/Z4/83tu15Kd0D1dycSE0ocwu9L+xw79fdbbl1yy1GbbQ6+6PssbmVV/ecnruXIpfwTVhyc/amJyp/64p+6fVso7n9sY3yFIVK/NkfqH+NmW1Ji6uuAskvOFsyt3vrtJDH68r13E/kz4frNGgzzZp929Hbb/GG38nvOU6BdsXrFSRHtyqgtAEW5JSczxdgqX4nXjtOJ5XoO4TftRom68r6//p8/k0fqH+nf2cnqP+U9dpk41B/KX1EB0/w/ibsynr6TlPj+Uxxmjt7sPn/Cqe4m/k/2/jr1q/94jGxu6ssDoqIijE7zuiV+dsL/fYtd2/n/r5bnPZeobsSC3DKTcvzEyl9zR5SZBC5SM0wZa/Tl5t/fzO0p817/cxQRv2Zlg/G2Osq3m8yaxN+5V4ILvSvszyfN7sL9R9mv4+fYOWJv2mu99dfc62PqWEpvLUWcazcxf8nlWn/65+v+Wg7nt/jaLHrzjr64of74LCiq/5XEvcfjBb0qnv5zuTXpNX66OVezT+h/MLc2f7QHDkWJ4WJqYqv4xdY2XZYxV5erAyefpeZ7hwCE2wJafYJ9bNKZnWfWH+OiVOT8zYqL4frlGzF+ap42s/6Ot4z339Re7pX6QplflmhGUNfgWu8p9PuVBvCXav4jHGaGcpvYrlee8q+0DwC+fTNfvU8bUfNK5YD9HCbacGvqdln/34F+8JKi1gni874fG7zQd0/RuLz9lu92//Gyxcnt6Qs9Xy1ymr9din8Xpvedk+jJQlG3tjT1NpKrunKe6Xw1ryU9oFXSdKR2g6zaRJk9S0aVMFBASoU6dOWrdunadL8rgDNt5wV/38v3E9T3+1WbvSPHM6b0IpY0vOdbPE0z8pl/WUxMl8l6au2lOm1xQp/qaQlJqjVT8fOse6yndvlrMNyi4eAqau2qu/Tokr0abwnIOTTYnfk7LepNIaCH4B3gyGzt4m6bTfF5vrLb4rfCuhWDun58YuKnlT1tKcb+Y4W1j+5bfyncIryxibspyq/PDH3W7jHSct/Vk9JpXvIoazsXOfpopkjFGfD9bokWkb3D7Q2d01R0+e2v5//Ddef5++3grCS5PSK+X0a0U4kVeoJ2duss5ieBNCUzFffPGFhgwZopdeekkbN25U+/btFR0drfR077364Ncjx/XVhhRb9+LYlZaj1+ZsV/y+DE1YvKvUOw/nFbi0+udDbm/ON41aUuryztYt/6e3zn6K42xyCwr1y++DRb9NKNt3wiWVEtbO1ePxVgWMR3n5++36+/QNSsk4LunUbRLs3Hyu+Cf56PEr1PfDtda2n+6NeTvUaugCfbPxV6XnnHtcyLb9Werz/hptSj4iP58z/6l/tPJ/ge+9FaX3GriMOWuvw5uLknTjyCXqMmapPlubLEkqLGMPnKcH/drtGStep18l9DTZOdt1oToZtu7P0v0frLF+r0tT1lOUZTnMdns4T+YX6rW5OzRqwU/67ff7g41ZmKTNKZkVfsl96VfPVd4RyS+2f8vzDQSbf83Sc19v0fxtqfphR7rVk9p/6nr96/NN1oed3IJCrdx16KwfzE7mF+rprzaf8S7/uQWFevbrzfp8XXKZ6yzuo5W79W3CAa+80z1f2FvMuHHj9Oijj6p///6SpClTpmju3Ln6+OOP9fzzz3ukptyCQrV8cYEk6bZWjbQrPUeHcvJ0zWUh6nP9Zfrn55skSc98vUXP3t5SLpfR/G2pSjyQrajWoXr81j8oLDhAktRt/AoZI334+5vkwsRUBdTw1cHME+raOlR7Dx/Tj7tO9XT4+/mUeqqruMv/M/+s81Myjmvy8l+UlnVSDodDQ/50hbJP5uvRTzboocgmiqhbU5+vS9bQP7dRwzr+OnI8X/VrOdX7/TXan3lCd7UP1/dn+CT0wIdr9ZerwxX5h/rasM/9BnhTV+1R84a19dDH6xQa5K+6NZ3WvMgRJU9pnD7W6duEA9qUnKmP+nVUoNNXvx45obwClyLq1dTCxFSlZZ9U84a1Syznhx1p2vJrpnpec4neX7FbjYMD9MXASOtTaGn3PerzwRrNjrlJxfNF17HL1aJRbT0U2UTXRNRVXqFLn8Tt1bcJp/bFkC83S5J+fPaPWpqUrqb1a+kPDWtZb0YpR45rx8EcvTpnuyTp7ndXn/WTcG6BS2MW/qTZmw6c8dTUnM0H9d7yU3do/s+drXV72zAlpeZo3raDOp5bqAW/b9vew8f1f7O2KqJeoGYn/O/Yzd1yUIu2p+rxW5urToCfjDkV6urW+t+x2Xf4uJIPH3d7g05KzdGa3YcV2by+fBwO/Zyeo8tD66jQZTRv60EVuoy6tg5V/d+XY8ypT/0uY6wQVDTtTFIyjqvQZbT/yHG3aUWvldxffzDrpPYeOiYj6ddivWu/HjluKxC8uShJG/Ye0aDbWpRYnyQlZxw/a0iRTu3n0/16pOS07QeyrWUVv+dR8eUfOZ6nkECnjIyMKdk7tfqXw+o1ebW+eCzSigbF98eho7nnrLe4lCPu21fgMmcMnwezTpS6bD9fh/IKXNb+Lv4BbsuvmWrdOMh6/stvR8tU37kcKXahQErGceUXutxOf+877H7/pNSsk8ovdFnjs3wcjlJ/H3898r9lrP75kCLq1ZTk3rtc/DsGf80s/fdkWVLJD/lfFLuL/cbkI/qp2Cn4hJRMuYzRf2Zt0/Kdv6n9pcF65/5rSxYoafLyX/R1/K/6Ov5XrXjmj5Lcfxc++PHUe8uXG37VJSGBataglqRTf48OOeTn6ygxTq3oQ0hadq7eX/GLHrmpmdbv/d8FK3sOHXP7/Qh0+qpBbf9S67sQHMYbvjHUC+Tl5almzZr6+uuv1bNnT2t6v379lJmZqW+//datfW5urnJz//cGk52drYiICGVlZSkoKEgV5dmvN+vLDZ4bIwQAgLf4S/twTehzTYUuMzs7W8HBwbbev+lp+t2hQ4dUWFio0NBQt+mhoaH66aeSl6qPGDFCL7/8cqXXde1ldc87NAXUOHVq5mT+hb0BTEANnwu+zopmp8ftbGo6fSWd+rR4plMNTj8fGWPcuuHLqugYG1P6YPiKdj7HNqDGqX1a2se1mk7fct3eIKCGj9spEh/H/07tOBwlTwmdOMfYsKL96eNw/N77Yty2t7a/nxyS8l0ua7rTz8fWFYPn2m9OX59Sx6DZrfn0dQTWOPU76DLG+t0o2l8FLpf1e1fL6SuH49RezCllHFBtfz/rFK2R+20oitZxNkX1O319VHS2uKjO4rWfXn/xeUXH+ER+oWo6feXQqWPkMkbHitXj6+Peo2GnPrvOdRxqOX3dapFObYOfj48KXK4z9nwW32ZfH4db70rRcfPzcajgtO06Vz3n4u/nI4ej5O9MaWPPircp+t9mzJn3SdF+9/U59bvmcpUcX+nrc+r4ne3vvvjfo9PPs6OKCE3l9MILL2jIkCHW86KeporW+/rL1Pv6yyp8uQAAoGwITb9r0KCBfH19lZbmfllnWlqawsLCSrT39/eXv7/nzqsCAIALi6vnfud0OtWhQwctXvy/gcIul0uLFy9WZGSkBysDAADegJ6mYoYMGaJ+/fqpY8eOuv766zV+/HgdO3bMupoOAABUX4SmYu677z799ttvGjZsmFJTU3X11VdrwYIFJQaHAwCA6odbDlSQslyyCAAAvENZ3r8Z0wQAAGADoQkAAMAGQhMAAIANhCYAAAAbCE0AAAA2EJoAAABsIDQBAADYQGgCAACwgdAEAABgA1+jUkGKbqyenZ3t4UoAAIBdRe/bdr4ghdBUQXJyciRJERERHq4EAACUVU5OjoKDg8/ahu+eqyAul0sHDhxQnTp15HA4KnTZ2dnZioiIUEpKCt9r52EcC+/AcfAeHAvvwbEoH2OMcnJyFB4eLh+fs49aoqepgvj4+OjSSy+t1HUEBQXxh+AlOBbegePgPTgW3oNjUXbn6mEqwkBwAAAAGwhNAAAANhCaqgB/f3+99NJL8vf393Qp1R7HwjtwHLwHx8J7cCwqHwPBAQAAbKCnCQAAwAZCEwAAgA2EJgAAABsITQAAADYQmrzcpEmT1LRpUwUEBKhTp05at26dp0uqUlasWKG77rpL4eHhcjgcmj17ttt8Y4yGDRumxo0bKzAwUFFRUdq1a5dbm4yMDPXt21dBQUEKCQnRgAEDdPToUbc2W7Zs0S233KKAgABFRERo9OjRJWr56quv1KpVKwUEBKhdu3aaN29ehW+vtxoxYoSuu+461alTR40aNVLPnj2VlJTk1ubkyZOKiYlR/fr1Vbt2bfXq1UtpaWlubZKTk9W9e3fVrFlTjRo10jPPPKOCggK3NsuWLdO1114rf39/tWjRQtOmTStRT3X+u5o8ebKuuuoq6waIkZGRmj9/vjWf4+A5I0eOlMPh0ODBg61pHA8vY+C1Zs6caZxOp/n4449NYmKiefTRR01ISIhJS0vzdGlVxrx588x//vMf88033xhJZtasWW7zR44caYKDg83s2bPN5s2bzV/+8hfTrFkzc+LECavN7bffbtq3b2/WrFljfvzxR9OiRQvTp08fa35WVpYJDQ01ffv2Ndu2bTOff/65CQwMNO+9957VZtWqVcbX19eMHj3abN++3bz44oumRo0aZuvWrZW+D7xBdHS0mTp1qtm2bZtJSEgwd955p7nsssvM0aNHrTaPP/64iYiIMIsXLzYbNmwwN9xwg7nxxhut+QUFBaZt27YmKirKbNq0ycybN880aNDAvPDCC1ab3bt3m5o1a5ohQ4aY7du3m4kTJxpfX1+zYMECq011/7v67rvvzNy5c83OnTtNUlKS+b//+z9To0YNs23bNmMMx8FT1q1bZ5o2bWquuuoq8+STT1rTOR7ehdDkxa6//noTExNjPS8sLDTh4eFmxIgRHqyq6jo9NLlcLhMWFmbGjBljTcvMzDT+/v7m888/N8YYs337diPJrF+/3mozf/5843A4zP79+40xxrz77rumbt26Jjc312rz3HPPmZYtW1rP7733XtO9e3e3ejp16mQee+yxCt3GqiI9Pd1IMsuXLzfGnNrvNWrUMF999ZXVZseOHUaSiYuLM8acCsA+Pj4mNTXVajN58mQTFBRk7ftnn33WXHnllW7ruu+++0x0dLT1nL+rkurWrWs+/PBDjoOH5OTkmMsvv9zExsaaW2+91QpNHA/vw+k5L5WXl6f4+HhFRUVZ03x8fBQVFaW4uDgPVnbx2LNnj1JTU932cXBwsDp16mTt47i4OIWEhKhjx45Wm6ioKPn4+Gjt2rVWm86dO8vpdFptoqOjlZSUpCNHjlhtiq+nqE11PZZZWVmSpHr16kmS4uPjlZ+f77aPWrVqpcsuu8ztWLRr106hoaFWm+joaGVnZysxMdFqc7b9zN+Vu8LCQs2cOVPHjh1TZGQkx8FDYmJi1L179xL7jOPhffjCXi916NAhFRYWuv0hSFJoaKh++uknD1V1cUlNTZWkUvdx0bzU1FQ1atTIbb6fn5/q1avn1qZZs2YlllE0r27dukpNTT3reqoTl8ulwYMH66abblLbtm0lndpPTqdTISEhbm1PPxal7cOieWdrk52drRMnTujIkSP8XUnaunWrIiMjdfLkSdWuXVuzZs1SmzZtlJCQwHG4wGbOnKmNGzdq/fr1Jebxd+F9CE0ALqiYmBht27ZNK1eu9HQp1VbLli2VkJCgrKwsff311+rXr5+WL1/u6bKqnZSUFD355JOKjY1VQECAp8uBDZye81INGjSQr69viask0tLSFBYW5qGqLi5F+/Fs+zgsLEzp6elu8wsKCpSRkeHWprRlFF/HmdpUt2M5aNAgzZkzR0uXLtWll15qTQ8LC1NeXp4yMzPd2p9+LMq7n4OCghQYGMjf1e+cTqdatGihDh06aMSIEWrfvr3efvttjsMFFh8fr/T0dF177bXy8/OTn5+fli9frgkTJsjPz0+hoaEcDy9DaPJSTqdTHTp00OLFi61pLpdLixcvVmRkpAcru3g0a9ZMYWFhbvs4Oztba9eutfZxZGSkMjMzFR8fb7VZsmSJXC6XOnXqZLVZsWKF8vPzrTaxsbFq2bKl6tata7Upvp6iNtXlWBpjNGjQIM2aNUtLliwpcTqzQ4cOqlGjhts+SkpKUnJystux2Lp1q1uIjY2NVVBQkNq0aWO1Odt+5u+qdC6XS7m5uRyHC6xr167aunWrEhISrEfHjh3Vt29f62eOh5fx9Eh0nNnMmTONv7+/mTZtmtm+fbsZOHCgCQkJcbtKAmeXk5NjNm3aZDZt2mQkmXHjxplNmzaZffv2GWNO3XIgJCTEfPvtt2bLli2mR48epd5y4JprrjFr1641K1euNJdffrnbLQcyMzNNaGioefDBB822bdvMzJkzTc2aNUvccsDPz8+8+eabZseOHeall16qVrcc+Mc//mGCg4PNsmXLzMGDB63H8ePHrTaPP/64ueyyy8ySJUvMhg0bTGRkpImMjLTmF11a3a1bN5OQkGAWLFhgGjZsWOql1c8884zZsWOHmTRpUqmXVlfnv6vnn3/eLF++3OzZs8ds2bLFPP/888bhcJhFixYZYzgOnlb86jljOB7ehtDk5SZOnGguu+wy43Q6zfXXX2/WrFnj6ZKqlKVLlxpJJR79+vUzxpy67cDQoUNNaGio8ff3N127djVJSUluyzh8+LDp06ePqV27tgkKCjL9+/c3OTk5bm02b95sbr75ZuPv728uueQSM3LkyBK1fPnll+aKK64wTqfTXHnllWbu3LmVtt3eprRjIMlMnTrVanPixAnzxBNPmLp165qaNWuau+++2xw8eNBtOXv37jV33HGHCQwMNA0aNDD//ve/TX5+vlubpUuXmquvvto4nU7zhz/8wW0dRarz39UjjzximjRpYpxOp2nYsKHp2rWrFZiM4Th42umhiePhXRzGGOOZPi4AAICqgzFNAAAANhCaAAAAbCA0AQAA2EBoAgAAsIHQBAAAYAOhCQAAwAZCEwAAgA2EJgAop6ZNm2r8+PGeLgPABUJoAlAlPPzww+rZs6ckqUuXLho8ePAFW/e0adMUEhJSYvr69es1cODAC1YHAM/y83QBAOApeXl5cjqd5X59w4YNK7AaAN6OniYAVcrDDz+s5cuX6+2335bD4ZDD4dDevXslSdu2bdMdd9yh2rVrKzQ0VA8++KAOHTpkvbZLly4aNGiQBg8erAYNGig6OlqSNG7cOLVr1061atVSRESEnnjiCR09elSStGzZMvXv319ZWVnW+oYPHy6p5Om55ORk9ejRQ7Vr11ZQUJDuvfdepaWlWfOHDx+uq6++Wp9++qmaNm2q4OBg9e7dWzk5OVabr7/+Wu3atVNgYKDq16+vqKgoHTt2rJL2JoCyIDQBqFLefvttRUZG6tFHH9XBgwd18OBBRUREKDMzU7fddpuuueYabdiwQQsWLFBaWpruvfdet9dPnz5dTqdTq1at0pQpUyRJPj4+mjBhghITEzV9+nQtWbJEzz77rCTpxhtv1Pjx4xUUFGSt7+mnny5Rl8vlUo8ePZSRkaHly5crNjZWu3fv1n333efW7pdfftHs2bM1Z84czZkzR8uXL9fIkSMlSQcPHlSfPn30yCOPaMeOHVq2bJnuuece8RWhgHfg9ByAKiU4OFhOp1M1a9ZUWFiYNf2dd97RNddcozfeeMOa9vHHHysiIkI7d+7UFVdcIUm6/PLLNXr0aLdlFh8f1bRpU7322mt6/PHH9e6778rpdCo4OFgOh8NtfadbvHixtm7dqj179igiIkKS9Mknn+jKK6/U+vXrdd1110k6Fa6mTZumOnXqSJIefPBBLV68WK+//roOHjyogoIC3XPPPWrSpIkkqV27duextwBUJHqaAFwUNm/erKVLl6p27drWo1WrVpJO9e4U6dChQ4nX/vDDD+ratasuueQS1alTRw8++KAOHz6s48eP217/jh07FBERYQUmSWrTpo1CQkK0Y8cOa1rTpk2twCRJjRs3Vnp6uiSpffv26tq1q9q1a6e//e1v+uCDD3TkyBH7OwFApSI0AbgoHD16VHfddZcSEhLcHrt27VLnzp2tdrVq1XJ73d69e/XnP/9ZV111lf7f//t/io+P16RJkySdGihe0WrUqOH23OFwyOVySZJ8fX0VGxur+fPnq02bNpo4caJatmypPXv2VHgdAMqO0ASgynE6nSosLHSbdu211yoxMVFNmzZVixYt3B6nB6Xi4uPj5XK5NHbsWN1www264oordODAgXOu73StW7dWSkqKUlJSrGnbt29XZmam2rRpY3vbHA6HbrrpJr388svatGmTnE6nZs2aZfv1ACoPoQlAldO0aVOtXbtWe/fu1aFDh+RyuRQTE6OMjAz16dNH69ev1y+//KKFCxeqf//+Zw08LVq0UH5+viZOnKjdu3fr008/tQaIF1/f0aNHtXjxYh06dKjU03ZRUVFq166d+vbtq40bN2rdunV66KGHdOutt6pjx462tmvt2rV64403tGHDBiUnJ+ubb77Rb7/9ptatW5dtBwGoFIQmAFXO008/LV9fX7Vp00YNGzZUcnKywsPDtWrVKhUWFqpbt25q166dBg8erJCQEPn4nPlfXfv27TVu3DiNGjVKbdu21YwZMzRixAi3NjfeeKMef/xx3XfffWrYsGGJgeTSqR6ib7/9VnXr1lXnzp0VFRWlP/zhD/riiy9sb1dQUJBWrFihO++8U1dccYVefPFFjR07VnfccYf9nQOg0jgM17ICAACcEz1NAAAANhCaAAAAbCA0AQAA2EBoAgAAsIHQBAAAYAOhCQAAwAZCEwAAgA2EJgAAABsITQAAADYQmgAAAGwgNAEAANhAaAIAALDh/wPB4SeqLhPHLgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_loss = pd.DataFrame({'iteration': range(1, len(training_losses) + 1), 'loss': training_losses})\n",
    "sns.lineplot(df_loss, x='iteration', y='loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7be7fac6-1e6b-4f44-a555-a518b6facab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 99.69%\n",
      "Test Accuracy: 96.46%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model accuracy\n",
    "def evaluate(data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.inference_mode():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "print(f\"Train Accuracy: {evaluate(train_loader):.2f}%\")    \n",
    "print(f\"Test Accuracy: {evaluate(test_loader):.2f}%\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "39eee766-4a80-452a-8afb-0f9fe066aa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9646    1.0000    0.9820      2777\n",
      "           1     0.0000    0.0000    0.0000       102\n",
      "\n",
      "    accuracy                         0.9646      2879\n",
      "   macro avg     0.4823    0.5000    0.4910      2879\n",
      "weighted avg     0.9304    0.9646    0.9472      2879\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kimbo/github-repos/chb-mit-scalp-eeg/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kimbo/github-repos/chb-mit-scalp-eeg/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kimbo/github-repos/chb-mit-scalp-eeg/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred_labels = torch.argmax(model(X_test_tensor.to(device)), dim=1).cpu()\n",
    "print(classification_report(y_test_tensor, y_pred_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7dca2ca8-4ae2-4586-8e55-3b7668f1f788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits (first 5): [[ 2.5422907e-01 -2.1144073e-01]\n",
      " [-1.3077927e+02 -3.8559232e+02]\n",
      " [-8.8514938e+01 -2.6128925e+02]\n",
      " [-2.7620047e+02 -8.1328876e+02]\n",
      " [ 2.5422907e-01 -2.1144073e-01]]\n",
      "Predictions: tensor([0, 0, 0, 0, 0], device='cuda:0')\n",
      "True labels: tensor([0, 0, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# inspecting logits for seeing why \n",
    "print(f\"Logits (first 5): {y_pred[:5].detach().cpu().numpy()}\")\n",
    "print(f\"Predictions: {torch.argmax(y_pred, dim=1)[:5]}\")\n",
    "print(f\"True labels: {labels[:5]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
